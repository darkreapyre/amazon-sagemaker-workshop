{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tqdm\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "import cv2\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from models.keras_ssd300 import ssd_300\n",
    "from ssd_utils import BBoxUtility\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Global Variables\n",
    "voc_classes = ['Aeroplane', 'Bicycle', 'Bird', 'Boat', 'Bottle',\n",
    "               'Bus', 'Car', 'Cat', 'Chair', 'Cow', 'Diningtable',\n",
    "               'Dog', 'Horse','Motorbike', 'Person', 'Pottedplant',\n",
    "               'Sheep', 'Sofa', 'Train', 'Tvmonitor']\n",
    "NUM_CLASSES = len(voc_classes) + 1\n",
    "bbox_util = BBoxUtility(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consofure SageMaker Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar -O /tmp/VOCtrainval_11-May-2012.tar\n",
    "!tar xf /tmp/VOCtrainval_11-May-2012.tar -C /tmp/\n",
    "!rm -rf /tmp/VOCtrainval_11-May-2012.tar\n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar -O /tmp/VOCtrainval_06-Nov-2007.tar\n",
    "!tar xf /tmp/VOCtrainval_06-Nov-2007.tar -C /tmp/\n",
    "!rm -rf /tmp/VOCtrainval_06-Nov-2007.tar\n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar -O /tmp/VOCtest_06-Nov-2007.tar\n",
    "!tar xf /tmp/VOCtest_06-Nov-2007.tar -C /tmp/\n",
    "!rm -rf /tmp/VOCtest_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default classes for VOCdevkit\n",
    "classes = ['background',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat',\n",
    "           'chair', 'cow', 'diningtable', 'dog',\n",
    "           'horse', 'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "NUM_CLASSES = len(classes) + 1\n",
    "\n",
    "# Initialize the Batch generator for datasets\n",
    "train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "val_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "\n",
    "# The directories that contain the images.\n",
    "VOC_2007_images_dir      = '/tmp/VOCdevkit/VOC2007/JPEGImages/'\n",
    "VOC_2012_images_dir      = '/tmp/VOCdevkit/VOC2012/JPEGImages/'\n",
    "\n",
    "# The directories that contain the annotations.\n",
    "VOC_2007_annotations_dir      = '/tmp/VOCdevkit/VOC2007/Annotations/'\n",
    "VOC_2012_annotations_dir      = '/tmp/VOCdevkit/VOC2012/Annotations/'\n",
    "\n",
    "# The paths to the image sets.\n",
    "VOC_2007_train_image_set_filename    = '/tmp/VOCdevkit/VOC2007/ImageSets/Main/train.txt'\n",
    "VOC_2012_train_image_set_filename    = '/tmp/VOCdevkit/VOC2012/ImageSets/Main/train.txt'\n",
    "VOC_2007_val_image_set_filename      = '/tmp/VOCdevkit/VOC2007/ImageSets/Main/val.txt'\n",
    "VOC_2012_val_image_set_filename      = '/tmp/VOCdevkit/VOC2012/ImageSets/Main/val.txt'\n",
    "VOC_2007_trainval_image_set_filename = '/tmp/VOCdevkit/VOC2007/ImageSets/Main/trainval.txt'\n",
    "VOC_2012_trainval_image_set_filename = '/tmp/VOCdevkit/VOC2012/ImageSets/Main/trainval.txt'\n",
    "VOC_2007_test_image_set_filename     = '/tmp/VOCdevkit/VOC2007/ImageSets/Main/test.txt'\n",
    "\n",
    "# Create Training Dataset\n",
    "train_dataset.parse_xml(images_dirs=[VOC_2007_images_dir,\n",
    "                                     VOC_2012_images_dir],\n",
    "                        image_set_filenames=[VOC_2007_trainval_image_set_filename,\n",
    "                                             VOC_2012_trainval_image_set_filename],\n",
    "                        annotations_dirs=[VOC_2007_annotations_dir,\n",
    "                                          VOC_2012_annotations_dir],\n",
    "                        classes=classes,\n",
    "                        include_classes='all',\n",
    "                        exclude_truncated=False,\n",
    "                        exclude_difficult=False,\n",
    "                        ret=False)\n",
    "\n",
    "# Create Testing Dataset\n",
    "val_dataset.parse_xml(images_dirs=[VOC_2007_images_dir],\n",
    "                      image_set_filenames=[VOC_2007_test_image_set_filename],\n",
    "                      annotations_dirs=[VOC_2007_annotations_dir],\n",
    "                      classes=classes,\n",
    "                      include_classes='all',\n",
    "                      exclude_truncated=False,\n",
    "                      exclude_difficult=True,\n",
    "                      ret=False)\n",
    "\n",
    "# Convert the dataset into an HDF5 dataset fpor S3 upload\n",
    "train_dataset.create_hdf5_dataset(file_path='/tmp/train.h5',\n",
    "                                  resize=False,\n",
    "                                  variable_image_size=True,\n",
    "                                  verbose=True)\n",
    "\n",
    "val_dataset.create_hdf5_dataset(file_path='/tmp/test.h5',\n",
    "                                resize=False,\n",
    "                                variable_image_size=True,\n",
    "                                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'data'\n",
    "channels = {'train': 's3://{}'.format(bucket)}\n",
    "sagemaker_session.upload_data(path='/tmp/train.h5', bucket=bucket, key_prefix=prefix)\n",
    "sagemaker_session.upload_data(path='/tmp/test.h5', bucket=bucket, key_prefix=prefix)\n",
    "hyperparameters = dict(learning_rate=.001, epochs=2)\n",
    "output_location = 's3://{}/output'.format(bucket)\n",
    "image_name = '500842391574.dkr.ecr.us-west-2.amazonaws.com/keras:keras-gpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BYOC_estimator = Estimator(\n",
    "    image_name,\n",
    "    role=role,\n",
    "    output_path=output_location,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p3.8xlarge',\n",
    "    hyperparameters=hyperparameters,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BYOC_estimator.fit(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "## SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for Boundary Boxes\n",
    "\n",
    "def get_bbox(xmin, ymin, xmax, ymax):\n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "def center_is_near(prev_bbox, bbox):\n",
    "    IS_NEAR_THRESHOLD = 30\n",
    "    \n",
    "    prev_center_x = (prev_bbox[0] + prev_bbox[2])/2.\n",
    "    prev_center_y = (prev_bbox[1] + prev_bbox[3])/2.\n",
    "    center_x = (bbox[0] + bbox[2])/2.\n",
    "    center_y = (bbox[1] + bbox[3])/2.\n",
    "    \n",
    "    dist = np.sqrt((prev_center_x - center_x)**2 + (prev_center_y - center_y)**2)\n",
    "    \n",
    "    if dist <= IS_NEAR_THRESHOLD:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def draw_boxes(img, preds, results):\n",
    "    global first_frame_has_car, prev_bboxes, prev_bboxes_len, bbox_disappear_frame_count\n",
    "    \n",
    "    # Parse the outputs.\n",
    "    det_label = results[0][:, 0]\n",
    "    det_conf = results[0][:, 1]\n",
    "    det_xmin = results[0][:, 2]\n",
    "    det_ymin = results[0][:, 3]\n",
    "    det_xmax = results[0][:, 4]\n",
    "    det_ymax = results[0][:, 5]\n",
    "\n",
    "    # Get detections with confidence higher than 0.6.\n",
    "    top_indices = [i for i, conf in enumerate(det_conf) if conf >= 0.6]\n",
    "\n",
    "    top_conf = det_conf[top_indices]\n",
    "    top_label_indices = det_label[top_indices].tolist()\n",
    "    top_xmin = det_xmin[top_indices]\n",
    "    top_ymin = det_ymin[top_indices]\n",
    "    top_xmax = det_xmax[top_indices]\n",
    "    top_ymax = det_ymax[top_indices]\n",
    "    \n",
    "    bboxes_len = 0\n",
    "    bboxes = []   \n",
    "    for i in range(top_conf.shape[0]):\n",
    "        \n",
    "        label = int(top_label_indices[i])        \n",
    "        label_name = voc_classes[label - 1]\n",
    "        \n",
    "        if label_name == 'Car':\n",
    "            bboxes_len += 1\n",
    "            \n",
    "            xmin = int(round(top_xmin[i] * img.shape[1]))\n",
    "            ymin = int(round(top_ymin[i] * img.shape[0]))\n",
    "            xmax = int(round(top_xmax[i] * img.shape[1]))\n",
    "            ymax = int(round(top_ymax[i] * img.shape[0]))\n",
    "            \n",
    "            if first_frame_has_car or len(prev_bboxes) == 0:\n",
    "                prev_bboxes.append(get_bbox(xmin, ymin, xmax, ymax))\n",
    "                first_frame_has_car = False\n",
    "                prev_bboxes_len = 0\n",
    "            else:\n",
    "                has_near_in_prev_bboxes = False\n",
    "                for i_prev_bbox in range(len(prev_bboxes)):\n",
    "                    if center_is_near(prev_bboxes[i_prev_bbox], [xmin, ymin, xmax, ymax]):\n",
    "                        ratiox = 0.5\n",
    "                        ratioy = 0.65\n",
    "                        xmin = int((1-ratiox)*xmin + ratiox*prev_bboxes[i_prev_bbox][0])\n",
    "                        ymin = int((1-ratioy)*ymin + ratioy*prev_bboxes[i_prev_bbox][1])\n",
    "                        xmax = int((1-ratiox)*xmax + ratiox*prev_bboxes[i_prev_bbox][2])\n",
    "                        ymax = int((1-ratioy)*ymax + ratioy*prev_bboxes[i_prev_bbox][3])\n",
    "                        prev_bboxes[i_prev_bbox][0] = xmin \n",
    "                        prev_bboxes[i_prev_bbox][1] = ymin\n",
    "                        prev_bboxes[i_prev_bbox][2] = xmax\n",
    "                        prev_bboxes[i_prev_bbox][3] = ymax\n",
    "                        has_near_in_prev_bboxes = True\n",
    "                        \n",
    "                if not has_near_in_prev_bboxes:\n",
    "                    prev_bboxes.append(get_bbox(xmin, ymin, xmax, ymax))\n",
    "                    \n",
    "            bboxes.append(get_bbox(xmin, ymin, xmax, ymax))  \n",
    "           \n",
    "    if prev_bboxes_len > bboxes_len and bbox_disappear_frame_count < 5:\n",
    "        for i_prev_bbox in range(len(prev_bboxes)):\n",
    "            for i_bbox in range(len(bboxes)):\n",
    "                if not center_is_near(prev_bboxes[i_prev_bbox], bboxes[i_bbox]):\n",
    "                    cv2.rectangle(img, \n",
    "                                  (prev_bboxes[i_prev_bbox][0],prev_bboxes[i_prev_bbox][1]), \n",
    "                                  (prev_bboxes[i_prev_bbox][2],prev_bboxes[i_prev_bbox][3]), (0,255,0), 5)\n",
    "            if len(bboxes) == 0:\n",
    "                cv2.rectangle(img, \n",
    "                              (prev_bboxes[i_prev_bbox][0],prev_bboxes[i_prev_bbox][1]), \n",
    "                              (prev_bboxes[i_prev_bbox][2],prev_bboxes[i_prev_bbox][3]), (0,255,0), 5)\n",
    "        bbox_disappear_frame_count += 1\n",
    "    else:\n",
    "        bbox_disappear_frame_count = 0\n",
    "        prev_bboxes_len = len(bboxes)\n",
    "        prev_bboxes = bboxes\n",
    "    for i_bbox in range(len(bboxes)):\n",
    "        cv2.rectangle(img, (bboxes[i_bbox][0],bboxes[i_bbox][1]), (bboxes[i_bbox][2],bboxes[i_bbox][3]), (0,255,0), 5)  \n",
    "            \n",
    "    if len(prev_bboxes) > 10:\n",
    "        prev_bboxes = []\n",
    "        bbox_disappear_frame_count = 10\n",
    "        \n",
    "    return img\n",
    "\n",
    "def process_video(input_img):\n",
    "    \n",
    "    inputs = []\n",
    "    #input_img_cropped = input_img[120:720,680:1280,:]\n",
    "    #img = cv2.resize(input_img_cropped, (300, 300))\n",
    "    img = cv2.resize(input_img, (300, 300))\n",
    "    img = image.img_to_array(img)\n",
    "    inputs.append(img.copy())\n",
    "    inputs = preprocess_input(np.array(inputs))\n",
    "    inputs = np.expand_dims(inputs[0], axis=0)\n",
    "    \n",
    "    preds = model.predict(inputs, batch_size=1, verbose=0)\n",
    "    results = bbox_util.detection_out(preds)\n",
    "    \n",
    "    final_img = draw_boxes(input_img, preds, results)\n",
    "    \n",
    "    return final_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_frame_has_car = True\n",
    "prev_bboxes = []\n",
    "bbox_disappear_frame_count = 0\n",
    "prev_bboxes_len = 0\n",
    "\n",
    "output = 'project_video_video_SSD_smooth_disappear.mp4'\n",
    "clip1 = VideoFileClip(\"/tmp/project_video.mp4\")\n",
    "clip = clip1.fl_image(process_video) #NOTE: this function expects color images!!\n",
    "%time clip.write_videofile(output, audio=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
